# Cloud Build Configuration which:
steps:
- name: 'gcr.io/cloud-builders/gcloud'
  id: terraform-google-initialize
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # create this resource if TPU, MIG or _SHARED_PD is not specified
    if [[ ${_BUILD_ACTION} == "initialize" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]] 
    then
      gcloud builds submit --config=./env_setup/initialize/initialize.yaml 
      wait
    # skip in all other conditions
    fi

# Create GCS Buckets
- name: 'gcr.io/cloud-builders/gcloud'
  id: terraform-google-gcs
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # create this resource if TPU, MIG or _SHARED_PD is not specified
    if [[ ${_BUILD_ACTION} == "create" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]] 
    then
      gcloud builds submit --config=./env_setup/terraform-google-gcs/terraform-google-gcs.yaml --substitutions _BUILD_ACTION=create
      wait
    # destroy this resource if TPU, MIG or _SHARED_PD is not specified
    elif [[ ${_BUILD_ACTION} == "destroy" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]] 
    then
      gcloud builds submit --config=./env_setup/terraform-google-gcs/terraform-google-gcs.yaml --substitutions _BUILD_ACTION=destroy
      wait 
    # skip in all other conditions
    fi 

# CREATE filestore
- name: 'gcr.io/cloud-builders/gcloud'
  id: terraform-google-filestore
  waitFor:
  - terraform-google-gcs
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # create this resource if TPU, MIG or _SHARED_PD is not specified
    if [[ ${_BUILD_ACTION} == "create" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]] 
    then
      gcloud builds submit --config=./env_setup/terraform-google-filestore/terraform-google-filestore.yaml --substitutions _BUILD_ACTION=create
      wait
    # destroy this resource if TPU, MIG or _SHARED_PD is not specified
    elif [[ ${_BUILD_ACTION} == "destroy" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-gcs/terraform-google-gcs.yaml --substitutions _BUILD_ACTION=destroy
      wait 
    # skip in all other conditions
    fi 

# Create CLOUD TPU 
- name: 'gcr.io/cloud-builders/gcloud'
  id: terraform-google-tpu
  waitFor:
    - terraform-google-gcs
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # create this resource if TPU is specified
    if [[ ${_BUILD_ACTION} == "update"  &&  ${_TPU} == "true" ]] || [[ ${_BUILD_ACTION} == "create" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-tpu/terraform-google-tpu.yaml --substitutions _BUILD_ACTION=create
      wait
    # destroy this resource if TPU specified
    elif [[ ${_BUILD_ACTION} == "destroy"  &&  ${_TPU} == "true" ]] || [[ ${_BUILD_ACTION} == "destroy" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-tpu/terraform-google-tpu.yaml --substitutions _BUILD_ACTION=destroy
      wait 
    # skip in all other conditions
    fi 

# Create MIG 
- name: 'gcr.io/cloud-builders/gcloud'
  id: terraform-google-mig
  waitFor:
    - terraform-google-filestore
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # create this resource if MIG is specified
    if [[ ${_BUILD_ACTION} == "update"  &&  ${_MIG} == "true" ]] || [[ ${_BUILD_ACTION} == "create" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-mig/terraform-google-mig.yaml --substitutions _BUILD_ACTION=create
      wait
    # destroy this resource if MIG specified
    elif [[ ${_BUILD_ACTION} == "destroy"  &&  ${_MIG} == "true" ]] || [[ ${_BUILD_ACTION} == "destroy" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-mig/terraform-google-mig.yaml --substitutions _BUILD_ACTION=destroy
      wait 
    # skip in all other conditions
    fi 

# Create SHARED_PD 
- name: 'gcr.io/cloud-builders/gcloud'
  id: terraform-google-disk
  waitFor:
    - terraform-google-mig
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # create this resource if SHARED_PD is specified
    if [[ ${_BUILD_ACTION} == "update"  &&  ${_SHARED_PD} == "true" ]] || [[ ${_BUILD_ACTION} == "create" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-disk/terraform-google-disk.yaml --substitutions _BUILD_ACTION=create
      wait
    # destroy this resource if SHARED_PD specified
    elif [[ ${_BUILD_ACTION} == "destroy"  &&  ${_SHARED_PD} == "true" ]] || [[ ${_BUILD_ACTION} == "destroy" && ${_TPU} == "default" && ${_MIG} == "default" && ${_SHARED_PD} == "default" ]]
    then
      gcloud builds submit --config=./env_setup/terraform-google-disk/terraform-google-disk.yaml --substitutions _BUILD_ACTION=destroy
      wait 
    # skip in all other conditions
    fi 

# # Load data to gcs
# - id: load-workspace-to-gcs
#   waitFor: 
#   - terraform-google-filestore-tpu
#   name: gcr.io/cloud-builders/gsutil
#   entrypoint: /bin/bash
#   args:
#     - -c
#     - |
#         set -xe
#         if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
#         then 
#           echo "WARNING - Skipping loading files to GCS"
#           exit 0
#         fi
#         trap cleanup EXIT
#         function cleanup {
#           if [ "$?" == "0" ]
#           then 
#             echo "Success! Configuration applied"
#             echo "exit 0" > /workspace/vars/exit_status.vars
#             exit 0
#           else
#             echo "exit 1" > /workspace/vars/exit_status.vars
#             exit 0
#           fi
#         }
#         sed -i "s|\(SHARED_FS=\)\(.*\)|\1${_SHARED_FS}|" scripts/setup_slaves.sh
#         sed -i "s|\(MOUNT_POINT=\)\(.*\)|\1${_MOUNT_POINT}|" scripts/setup_slaves.sh
#         sed -i "s|\(PYTORCH_PROJ_NAME=\)\(.*\)|\1${_PROJECT_ID}|" scripts/setup_slaves.sh
#         sed -i "s|\(BUILD=\)\(.*\)|\1${BUILD_ID}|" scripts/setup_slaves.sh
#         sed -i "s|\(USERNAME=\)\(.*\)|\1${_USERNAME}|" scripts/setup_slaves.sh

#         sed -i "s@\(SHARED_FS =[ ']*\)[^']*\(.\)@\1${_SHARED_FS}\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
#         sed -i "s@\(MOUNT_POINT =[ ']*\)[^']*\(.\)@\1${_MOUNT_POINT}\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
#         sed -i "s@\(PYTORCH_PROJ_NAME =[ ']*\)[^']*\(.\)@\1${_PROJECT_ID}\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
#         sed -i "s@\(BUILD =[ ']*\)[^']*\(.\)@\1${BUILD_ID}\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
#         sed -i "s@\(MOUNT_POINT =[ ']*\)[^']*\(.\)@\1${_MOUNT_POINT}\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
#         sed -i "s@\(TPU_POD_NAME =[ ']*\)[^']*\(.\)@\1$(cat /workspace/vars/tpu_name.vars)\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
#         sed -i "s@\(NFS_IP =[ ']*\)[^']*\(.\)@\1$(cat /workspace/vars/nfs_ip.vars)\2@" scripts/PyTorch_RoBERTa_CloudTPU.ipynb

#         gsutil -m cp -r scripts/* gs://${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-workspace

#     # TODO Replace remote-builder with custom workers currently in (alpha)[https://cloud.google.com/cloud-build/docs/custom-worker-pool] 
# - id: tpu-docker-images-seed-nfs
#   name: gcr.io/$PROJECT_ID/remote-builder
#   waitFor: 
#   - load-workspace-to-gcs
#   timeout: 2500s
#   env:
#     # Install nfs, docker in worker host
#     - COMMAND1=sudo apt-get -y install nfs-common curl && sudo curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh && sudo usermod -aG docker ${_USERNAME}
#     # Mount the NFS share to workspace
#     - COMMAND2=sudo mkdir -p ${_MOUNT_POINT} && sudo mount -t nfs -o nfsvers=3 ${_NFS_IP}:/${_SHARED_FS} ${_MOUNT_POINT} &&  sudo chmod go+rw ${_MOUNT_POINT}
#     # Copy the Roberta files to dataset
#     - COMMAND3=if [ -d "${_MOUNT_POINT}/code" ]; then echo "using existing ${_MOUNT_POINT}/code directory"; else sudo mkdir -p ${_MOUNT_POINT}/code && sudo git clone ${_CODE_REPO} ${_MOUNT_POINT}/code/; fi
#     # Copy the PyTorch_RoBERTa_CloudTPU.ipynb files and setup_slaves.sh files
#     - COMMAND4=if [ -d "${_MOUNT_POINT}/scripts" ]; then echo "using existing ${_MOUNT_POINT}/scripts directory"; else sudo mkdir -p ${_MOUNT_POINT}/scripts && sudo gsutil -m cp -r gs://${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-workspace ${_MOUNT_POINT}/scripts/; fi
#     # Create the dataset folder and download the dataset
#     - COMMAND5=if [ -d "${_MOUNT_POINT}/data" ]; then echo "using existing ${_MOUNT_POINT}/data directory"; else sudo mkdir -p ${_MOUNT_POINT}/data && sudo gsutil -m cp -r ${_GCS_DATASET} ${_MOUNT_POINT}/data/; fi && cat cat /etc/mtab | grep -i ${_MOUNT_POINT} |sudo tee -a /etc/fstab && sudo umount ${_MOUNT_POINT}
#     # Create the docker NFS volume
#     - COMMAND6=docker volume create --driver local --opt type=nfs --opt o=addr=${_NFS_IP} --opt device=:/${_SHARED_FS} ${_SHARED_FS}
#     # Build the appropriate nightly container, pull it and copy the startup script into to workspace 
#     - COMMAND7=df -kh && cd /home/${_USERNAME}/workspace/ && docker build --build-arg IMAGE_NIGHTLY=${_DOCKER_IMAGE_VERSION} --tag gcr.io/$PROJECT_ID/xla:$BUILD_ID .
#     # TODO Manual docker registry to address issues related to docker-credential-gcr not working properly
#     - COMMAND8=sudo docker login -u oauth2accesstoken -p "$(sudo gcloud auth print-access-token)" https://gcr.io
#     # Run the container : Copy files from GCS, clone the taylan@ roberta repo. Push the container to repo with a tag 
#     - COMMAND9=cd /home/${_USERNAME}/workspace/ && docker run -t -v ${_SHARED_FS}:${_MOUNT_POINT} gcr.io/$PROJECT_ID/xla:$BUILD_ID ./setup_nightly.sh ${_MOUNT_POINT} && sudo docker push gcr.io/$PROJECT_ID/xla:$BUILD_ID
#     - ZONE=${_ZONE}
#     # TODO: Service account. Add service account so that you can set nessaary on permissions from terraform
#     # TODO: Add ability to change TPU runtime if set
#     - INSTANCE_ARGS=--image debian-9-torch-xla-v${_GCE_IMAGE_VERSION} --image-project ml-images --boot-disk-type=pd-ssd --machine-type n1-standard-16 --tags=allow-ssh --boot-disk-size=200 --scopes=storage-rw,logging-write,monitoring,service-management,pubsub,service-control,trace
#     - USERNAME=${_USERNAME}
#     # Create GCE instances
# - id: terraform-google-instances
#   name: hashicorp/terraform:0.12.16
#   waitFor: 
#   - tpu-docker-images-seed-nfs
#   entrypoint: 'bash'
#   args: 
#   - '-c'
#   - | 
#       set -xe
#       function cleanupnfstpu {
#         if [ "$(cat /workspace/vars/exit_status_nfs_tpu.vars)" == "exit 0" ]
#         then 
#           cd env_setup/terraform-google-filestore-tpu
#           terraform init \
#             -backend-config="bucket=${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-backend" \
#             -backend-config="prefix=terraform-google-filestore-tpu"
#           terraform destroy -auto-approve \
#             -var="project_id=${_PROJECT_ID}" \
#             -var="region=${_REGION}" \
#             -var="zone=${_ZONE}" 
#         fi
#       }
#       function cleanup {
#         if [ "$?" != "0" ] || [ "$(cat /workspace/vars/total_cleanup.vars)" == "exit 1" ]
#         then 
#           cleanupnfstpu
#           terraform init \
#             -backend-config="bucket=${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-backend" \
#             -backend-config="prefix=terraform-google-instances"
#           terraform destroy -auto-approve \
#             -var="project_id=${_PROJECT_ID}" \
#             -var="region=${_REGION}" \
#             -var="zone=${_ZONE}"
#           exit 0
#         else
#            terraform output compute_instances_slave_0 > /workspace/vars/compute_instances_slave_0.vars
#         fi 
#       }
#       if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
#       then 
#         echo "exit 1" > /workspace/vars/total_cleanup.vars
#         cleanup
#         exit 0
#       fi
#       trap cleanup EXIT
#       if [ -d "env_setup/terraform-google-instances/" ]; then
#         cd env_setup/terraform-google-instances
#         terraform init \
#             -backend-config="bucket=${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-backend" \
#             -backend-config="prefix=terraform-google-instances"
#         terraform plan \
#             -var="project_id=${_PROJECT_ID}" \
#             -var="region=${_REGION}" \
#             -var="zone=${_ZONE}" \
#             -var="pytorch_proj_name=${_PROJECT_ID}" \
#             -var="accelerator_type=${_ACCELERATOR_TYPE}" \
#             -var="tpu_name=${_ENV_BUILD_NAME}" \
#             -var="nightly_image=${_DOCKER_IMAGE_VERSION}" \
#             -var="machine_type=${_MACHINE_TYPE}" \
#             -var="source_image_project=${_PROJECT_ID}" \
#             -var="source_image=${_GCE_IMAGE_NAME}" \
#             -var="disk_size_gb=${_GCE_DISK_SIZE}" \
#             -var="ports=${_PORTS}" \
#             -var="source_ranges=${_SOURCE_RANGES}" \
#             -var="tags=${_TAGS}"
#         terraform apply -auto-approve \
#             -var="project_id=${_PROJECT_ID}" \
#             -var="region=${_REGION}" \
#             -var="zone=${_ZONE}" \
#             -var="pytorch_proj_name=${_PROJECT_ID}" \
#             -var="accelerator_type=${_ACCELERATOR_TYPE}" \
#             -var="tpu_name=${_ENV_BUILD_NAME}" \
#             -var="nightly_image=${_DOCKER_IMAGE_VERSION}" \
#             -var="machine_type=${_MACHINE_TYPE}" \
#             -var="source_image_project=${_PROJECT_ID}" \
#             -var="source_image=${_GCE_IMAGE_NAME}" \
#             -var="disk_size_gb=${_GCE_DISK_SIZE}" \
#             -var="ports=${_PORTS}" \
#             -var="source_ranges=${_SOURCE_RANGES}" \
#             -var="tags=${_TAGS}"
#       fi
#  # Load data to gcs
# - id: clean-up-gcs
#   waitFor: 
#   - terraform-google-instances
#   name: gcr.io/cloud-builders/gsutil
#   entrypoint: /bin/bash
#   args:
#     - -c
#     - |
#         if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
#         then
#           gsutil rm -r gs://${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-backend
#           gsutil rm -r gs://${_PROJECT_ID}-${_ENV_BUILD_NAME}-tpu-workspace
#         else
#           echo -e "\033[0;32m export NFS_IP=$(cat /workspace/vars/nfs_ip.vars) \033[0m"
#           echo -e "\033[0;32m export TPU_POD_NAME=$(cat /workspace/vars/tpu_name.vars) \033[0m"
#           echo -e "\033[0;32m export MOUNT_POINT=${_MOUNT_POINT} \033[0m"
#           echo -e "\033[0;32m export SHARED_FS=${_SHARED_FS} \033[0m"
#           echo -e "\033[0;32m export BUILD=$BUILD_ID \033[0m"
#           echo -e "\033[0;32m export PYTORCH_PROJ_NAME=${_PROJECT_ID} \033[0m"
#           echo -e "\033[0;32m Jupyter URL-> http://$(gcloud compute instances describe ${_GCE_SLAVE_0} --zone ${_ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'):${_PORTS} \033[0m"
#           echo -e "\033[0;32m Jupyter PASSWORD->  $BUILD_ID \033[0m"
#         fi
substitutions:
#     _USERNAME: sivaibhav
#     # TODO: Service account.  Add service account to replace the default sv account 
#     _MOUNT_POINT: /mnt/common 
#     _SHARED_FS: tpushare # needs to be between 7 - 16 characters
#     _ZONE: europe-west4-a
#     _REGION: europe-west4
#     _PROJECT_ID: pytorch-tpu-new
#     _DOCKER_IMAGE_VERSION: r1.5 # Docker Container version: takes the form nightly_YYYYMMDD,  nightly or r1.5 for latest stable version
#     _GCE_IMAGE_VERSION: '20200429' #GCE Image Version: takes the form YYYYMMDD
#     _GCS_DATASET: gs://tpu-demo-eu/dataset/*
#     _NFS_IP: $(cat ~/workspace/vars/nfs_ip.vars) # variable file in the GCE builder instance
#     _GCE_IMAGE_NAME: $(cat /workspace/vars/gce_image_name.vars)
#     _GCE_SLAVE_0: $(cat /workspace/vars/compute_instances_slave_0.vars)
#     _CODE_REPO: https://github.com/taylanbil/fairseq.git 
#     _ACCELERATOR_TYPE: v3-32
#   #  _TPU_CIDR_BLOCK: "10.4.0.0/29"
#   #  _PYTORCH_VERSION: pytorch-1.5  # TPU runtime version: takes the form pytorch-nightly, pytorch-1.5 
#     _MACHINE_TYPE: n1-standard-16
#     _GCE_DISK_SIZE: '200'
#     _PORTS: '8888'
#     _SOURCE_RANGES: 0.0.0.0/0
#     _TAGS: jupyter
#     _ENV_BUILD_NAME: 'v20200428' # unique name, starts with a lowercase character
    _BUILD_ACTION: default
    _TPU: default
    _MIG: default
    _SHARED_PD: default
timeout: 4000s


# TODO: Ability to set TPU Runtime when specified 

# TODO: Cloud build to refresh instances based on changes to github

# TODO: CLoud build TF to turn off enviroment and save data to GCS bucket, 

# TODO: add ability to trigger changes, watching the github checkin 

# TODO: add ability to cache run


