# Cloud Build Configuration which:
# (1) Builds, tests, and pushes gcr.io/$PROJECT_ID/xla:$BUILD_ID RoBERTa image

steps:
# Create GCS Buckets
- id: terraform-google-gcs
  name: hashicorp/terraform:0.12.16
  entrypoint: 'sh'
  args: 
  - '-c'
  - |  
      set -xe
      trap cleanup EXIT
      function cleanup {
          if [ "$?" == "0" ]
          then 
            echo "Success! Configuration applied"
            echo $(terraform output backend_bucket_url) > /workspace/vars/backend_bucket_url.vars
            echo $(terraform output script_bucket_url) > /workspace/vars/script_bucket_url.vars
            echo "exit 0" > /workspace/vars/exit_status.vars
          else
            echo "exit 1" > /workspace/vars/exit_status.vars
            echo "exit 0" > /workspace/vars/exit_status_nfs_tpu.vars
            exit 0
          fi
      }
      mkdir -p vars/
      if [ -d "modules/terraform-google-gcs/" ]; then
        cd modules/terraform-google-gcs  
        terraform init 
        terraform plan \
            -var="project_id=${_PROJECT_ID}" \
            -var="pytorch_proj_name=${_PROJECT_ID}" \
            -var="gcs_tf_backend=${_GCS_TF_BACKEND_URL}" \
            -var="gcs_scripts=${_GCS_SCRIPT_URL}" 
        terraform apply -auto-approve \
            -var="project_id=${_PROJECT_ID}" \
            -var="pytorch_proj_name=${_PROJECT_ID}" \
            -var="gcs_tf_backend=${_GCS_TF_BACKEND_URL}" \
            -var="gcs_scripts=${_GCS_SCRIPT_URL}" 
      fi 
      
# Create TPU, Filestore 
- id: terraform-google-filestore-tpu
  name: hashicorp/terraform:0.12.16
  waitFor: 
  - terraform-google-gcs
  entrypoint: 'sh'
  args: 
  - '-c'
  - |  
      set -xe
      if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
      then 
        echo "exit 1" > /workspace/vars/exit_status.vars
        echo "WARNING - Skipping creating TPU and Filestore"
        exit 0
      fi
      trap cleanup EXIT
      function cleanup {
        if [ "$?" == "0" ]
        then 
          echo "Success! Configuration applied"
          terraform output nfs_ip > /workspace/vars/nfs_ip.vars
          terraform output default_account > /workspace/vars/default_account.vars
          terraform output tpu_name > /workspace/vars/tpu_name.vars
          terraform output tpu_project > /workspace/vars/tpu_project.vars
          terraform output tpu_zone > /workspace/vars/tpu_zone.vars
        else
          echo "exit 1" > /workspace/vars/exit_status.vars
          terraform destroy -auto-approve \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}" 
          exit 0
        fi
      }
      if [ -d "modules/terraform-google-filestore-tpu/" ]; then
        cd modules/terraform-google-filestore-tpu
        terraform init \
            -backend-config="bucket=${_GCS_TF_BACKEND_URL}" \
            -backend-config="prefix=terraform-google-filestore-tpu"
        terraform plan \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}" \
            -var="pytorch_proj_name=${_PROJECT_ID}" \
            -var="accelerator_type=${_ACCELERATOR_TYPE}" \
            -var="nightly_image=${_IMAGE_NIGHTLY}" \
            -var="machine_type=${_MACHINE_TYPE}" \
            -var="script_bucket_url=${_GCS_SCRIPT_URL}" \
            -var="tpu_shares_name=${_SHARED_FS}"
        terraform apply -auto-approve \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}" \
            -var="pytorch_proj_name=${_PROJECT_ID}" \
            -var="accelerator_type=${_ACCELERATOR_TYPE}" \
            -var="nightly_image=${_IMAGE_NIGHTLY}" \
            -var="machine_type=${_MACHINE_TYPE}" \
            -var="script_bucket_url=${_GCS_SCRIPT_URL}" \
            -var="tpu_shares_name=${_SHARED_FS}"
        echo "exit 0" > /workspace/vars/exit_status_nfs_tpu.vars
      fi 

      
  # Load data to gcs
- id: load-scripts-to-gcs
  waitFor: 
  - terraform-google-filestore-tpu
  name: gcr.io/cloud-builders/gsutil
  entrypoint: /bin/bash
  args:
    - -c
    - |
        set -xe
        if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
        then 
          echo "WARNING - Skipping loading files to GCS"
          exit 0
        fi
        trap cleanup EXIT
        function cleanup {
          if [ "$?" == "0" ]
          then 
            echo "Success! Configuration applied"
            echo "exit 0" > /workspace/vars/exit_status.vars
            exit 0
          else
            echo "exit 1" > /workspace/vars/exit_status.vars
            exit 0
          fi
        }
        gsutil -m cp -r scripts/* gs://${_GCS_SCRIPT_URL}

    # TODO Replace remote-builder with custom workers currently in (alpha)[https://cloud.google.com/cloud-build/docs/custom-worker-pool] 
- id: tpu-docker-images-seed-nfs
  name: gcr.io/$PROJECT_ID/remote-builder
  waitFor: 
  - load-scripts-to-gcs
  timeout: 2500s
  env:
    # Install nfs, docker in worker host
    - COMMAND1=sudo apt-get -y install nfs-common curl && sudo curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh && sudo usermod -aG docker ${_USERNAME}
    # Mount the NFS share to workspace
    - COMMAND2=sudo mkdir -p ${_MOUNT_POINT} && sudo mount -t nfs -o nfsvers=3 ${_NFS_IP}:/${_SHARED_FS} ${_MOUNT_POINT} &&  sudo chmod go+rw ${_MOUNT_POINT} && df -kh && ls -al ${_MOUNT_POINT}
    # Copy the Roberta files to dataset
    - COMMAND3=if [ -d "${_MOUNT_POINT}/code" ]; then echo "using existing ${_MOUNT_POINT}/code directory"; else sudo mkdir -p ${_MOUNT_POINT}/code && sudo git clone ${_CODE_REPO} ${_MOUNT_POINT}/code/; fi
    # Create the dataset folder and download the dataset
    - COMMAND4=if [ -d "${_MOUNT_POINT}/data" ]; then echo "using existing ${_MOUNT_POINT}/data directory"; else sudo mkdir -p ${_MOUNT_POINT}/data && sudo gsutil -m cp -r ${_GCS_DATASET} ${_MOUNT_POINT}/data/; fi && sudo umount ${_MOUNT_POINT}
    # Create the docker NFS volume
    - COMMAND5=docker volume create --driver local --opt type=nfs --opt o=addr=${_NFS_IP} --opt device=:/${_SHARED_FS} ${_SHARED_FS}
    # Build the appropriate nightly container, pull it and copy the startup script into to workspace 
    - COMMAND6=df -kh && cd /home/${_USERNAME}/workspace/ && docker build --build-arg IMAGE_NIGHTLY=${_IMAGE_NIGHTLY} --tag gcr.io/$PROJECT_ID/xla:$BUILD_ID .
    # TODO Manual docker registry to address issues related to docker-credential-gcr not working properly
    - COMMAND7=sudo docker login -u oauth2accesstoken -p "$(sudo gcloud auth print-access-token)" https://gcr.io
    # Run the container : Copy files from GCS, clone the taylan@ roberta repo. Push the container to repo with a tag 
    - COMMAND8=cd /home/${_USERNAME}/workspace/ && docker run -t -v ${_SHARED_FS}:${_MOUNT_POINT} gcr.io/$PROJECT_ID/xla:$BUILD_ID ./setup_nightly.sh ${_MOUNT_POINT} && sudo docker push gcr.io/$PROJECT_ID/xla:$BUILD_ID
    - ZONE=${_ZONE}
    # TODO: Service account. Add service account so that you can set nessaary on permissions from terraform
    # TODO: Add ability to change TPU runtime if set
    - INSTANCE_ARGS=--image-project ubuntu-os-cloud --image-family ubuntu-1804-lts --boot-disk-type=pd-ssd --machine-type n1-standard-16 --tags=allow-ssh --boot-disk-size=200 --scopes=storage-rw,logging-write,monitoring,service-management,pubsub,service-control,trace
    - USERNAME=${_USERNAME}
    # Create GCE instances
- id: terraform-google-instances
  name: hashicorp/terraform:0.12.16
  waitFor: 
  - tpu-docker-images-seed-nfs
  entrypoint: 'sh'
  args: 
  - '-c'
  - | 
      set -xe
      function cleanupnfstpu {
        if [ "$(cat /workspace/vars/exit_status_nfs_tpu.vars)" == "exit 0" ]
        then 
          cd modules/terraform-google-filestore-tpu
          terraform init \
            -backend-config="bucket=${_GCS_TF_BACKEND_URL}" \
            -backend-config="prefix=terraform-google-filestore-tpu"
          terraform destroy -auto-approve \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}" 
        fi
      }
      function cleanup {
        if [ "$?" != "0" ] || [ "$(cat /workspace/vars/total_cleanup.vars)" == "exit 1" ]
        then 
          cleanupnfstpu
          terraform init \
            -backend-config="bucket=${_GCS_TF_BACKEND_URL}" \
            -backend-config="prefix=terraform-google-instances"
          terraform destroy -auto-approve \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}"
          exit 0
        else
           terraform output compute_instances_slave_0 > /workspace/vars/compute_instances_slave_0.vars
        fi 
      }
      if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
      then 
        echo "exit 1" > /workspace/vars/total_cleanup.vars
        cleanup
        exit 0
      fi
      trap cleanup EXIT
      if [ -d "modules/terraform-google-instances/" ]; then
        sed -i "s|\(SHARED_FS=\)\(.*\)|\1${_SHARED_FS}|" scripts/setup_slaves.sh
        sed -i "s|\(MOUNT_POINT=\)\(.*\)|\1${_MOUNT_POINT}|" scripts/setup_slaves.sh
        sed -i "s|\(PYTORCH_PROJ_NAME=\)\(.*\)|\1${_PROJECT_ID}|" scripts/setup_slaves.sh
        sed -i "s|\(BUILD=\)\(.*\)|\1${BUILD_ID}|" scripts/setup_slaves.sh

        sed -i "s|\(SHARED_FS=\)\(.*\)|\1${_SHARED_FS}|" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
        sed -i "s|\(MOUNT_POINT=\)\(.*\)|\1${_MOUNT_POINT}|" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
        sed -i "s|\(PYTORCH_PROJ_NAME=\)\(.*\)|\1${_PROJECT_ID}|" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
        sed -i "s|\(BUILD=\)\(.*\)|\1${BUILD_ID}|" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
        sed -i "s|\(TPU_POD_NAME=\)\(.*\)|\1$(cat /workspace/vars/tpu_name.vars)|" scripts/PyTorch_RoBERTa_CloudTPU.ipynb
        sed -i "s|\(NFS_IP=\)\(.*\)|\1$(cat /workspace/vars/nfs_ip.vars)|" scripts/PyTorch_RoBERTa_CloudTPU.ipynb

        cd modules/terraform-google-instances
        terraform init \
            -backend-config="bucket=${_GCS_TF_BACKEND_URL}" \
            -backend-config="prefix=terraform-google-instances"
        terraform plan \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}" \
            -var="pytorch_proj_name=${_PROJECT_ID}" \
            -var="accelerator_type=${_ACCELERATOR_TYPE}" \
            -var="nightly_image=${_IMAGE_NIGHTLY}" \
            -var="machine_type=${_MACHINE_TYPE}" \
            -var="source_image_project=${_PROJECT_ID}" \
            -var="source_image=${_GCE_IMAGE_NAME}" \
            -var="disk_size_gb=${_GCE_DISK_SIZE}" \
            -var="ports=${_PORTS}" \
            -var="source_ranges=${_SOURCE_RANGES}" \
            -var="tags=${_TAGS}"
        terraform apply -auto-approve \
            -var="project_id=${_PROJECT_ID}" \
            -var="region=${_REGION}" \
            -var="zone=${_ZONE}" \
            -var="pytorch_proj_name=${_PROJECT_ID}" \
            -var="accelerator_type=${_ACCELERATOR_TYPE}" \
            -var="nightly_image=${_IMAGE_NIGHTLY}" \
            -var="machine_type=${_MACHINE_TYPE}" \
            -var="source_image_project=${_PROJECT_ID}" \
            -var="source_image=${_GCE_IMAGE_NAME}" \
            -var="disk_size_gb=${_GCE_DISK_SIZE}" \
            -var="ports=${_PORTS}" \
            -var="source_ranges=${_SOURCE_RANGES}" \
            -var="tags=${_TAGS}"
      fi
 # Load data to gcs
- id: clean-up-gcs
  waitFor: 
  - terraform-google-instances
  name: gcr.io/cloud-builders/gsutil
  entrypoint: /bin/bash
  args:
    - -c
    - |
        if [ "$(cat /workspace/vars/exit_status.vars)" != "exit 0" ]
        then
          gsutil rm -r gs://${_GCS_TF_BACKEND_URL}
          gsutil rm -r gs://${_GCS_SCRIPT_URL}
        else
          echo -e "\033[0;32m export NFS_IP=$(cat /workspace/vars/nfs_ip.vars) \033[0m"
          echo -e "\033[0;32m export TPU_POD_NAME=$(cat /workspace/vars/tpu_name.vars) \033[0m"
          echo -e "\033[0;32m export MOUNT_POINT=${_MOUNT_POINT} \033[0m"
          echo -e "\033[0;32m export SHARED_FS=${_SHARED_FS} \033[0m"
          echo -e "\033[0;32m export BUILD=$BUILD_ID \033[0m"
          echo -e "\033[0;32m export PYTORCH_PROJ_NAME=${_PROJECT_ID} \033[0m"
          echo -e "\033[0;32m Jupyter URL:http://$(gcloud compute instances describe ${_GCE_SLAVE_0} --zone ${_ZONE} --format='get(networkInterfaces[0].accessConfigs[0].natIP)'):${_PORTS} = \033[0m"
          echo -e "\033[0;32m Jupyter PASSWORD:$BUILD_ID \033[0m"
        fi
substitutions:
    _USERNAME: isaackkaranja
    # TODO: Service account.  Add service account to replace the default sv account 
    _MOUNT_POINT: /mnt/common 
    _SHARED_FS: tpushare # needs to be between 7 - 16 characters
    _ZONE: europe-west4-a
    _REGION: europe-west4
    _PROJECT_ID: pytorch-tpu-new
    _IMAGE_NIGHTLY: ""
    _GCS_DATASET: gs://pytorch-tpu-nfs-dataset/dataset/*
    _NFS_IP: $(cat ~/workspace/vars/nfs_ip.vars) # variable file in the GCE builder instance
    _GCE_IMAGE_NAME: $(cat /workspace/vars/gce_image_name.vars)
    _GCE_SLAVE_0: $(cat /workspace/vars/compute_instances_slave_0.vars)
    _CODE_REPO: https://github.com/taylanbil/fairseq.git 
    _ACCELERATOR_TYPE: v3-32
    _GCS_SCRIPT_URL : ${PYTORCH_PROJ_NAME}-tpu-scripts
    _GCS_TF_BACKEND_URL : ${PYTORCH_PROJ_NAME}-tpu-backend
    _MACHINE_TYPE: n1-standard-16
    _GCE_DISK_SIZE: '200'
    _PORTS: '8888'
    _SOURCE_RANGES: 0.0.0.0/0
    _TAGS: jupyter
options: 
    env:
      'PYTORCH_PROJ_NAME=nyc-isaack'
timeout: 4000s


# TODO: Ability to set TPU Runtime when specified 

# TODO: Cloud build to refresh instances based on changes to github
## -  Entire worker

# TODO: CLoud build TF to turn off enviroment and save data to GCS bucket, 

# TODO: add ability to trigger changes, watching the github checkin 

# TODO: add ability to cache run


